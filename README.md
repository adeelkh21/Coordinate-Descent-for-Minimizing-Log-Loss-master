# Exploring Coordinate Descent Optimization  
### Theory, Implementation, and Applications  

## Overview  
This project explores **Coordinate Descent (CD)**, a powerful optimization technique known for its simplicity and scalability in high-dimensional settings. We analyze its theoretical foundations, implement multiple variants, and compare its performance with other optimization methods, such as **Gradient Descent and Logistic Regression**.  

Using the **candy-data.csv** dataset from Kaggle, we apply Coordinate Descent to optimize logistic regression for binary classification. The implementation includes:  
- **Preprocessing and feature engineering** (handling categorical variables, normalization, train-test split)  
- **Implementation of Coordinate Descent for Logistic Regression**  
- **Comparison with other optimization techniques**  
- **Loss tracking and visualization** to analyze convergence behavior  

## Key Findings  
- CD provides a computationally efficient approach for optimization, particularly in high-dimensional settings.  
- It performs well in many cases but may exhibit slower convergence compared to methods like **Gradient Descent**.  
- The study highlights CDâ€™s advantages, limitations, and real-world applications.  

## Technologies Used  
- **Python** (NumPy, Pandas, Matplotlib, Seaborn)  
- **Scikit-learn** (for benchmarking against traditional Logistic Regression)  

## Contributors  
- **Nauman Ali Murad** (Reg# 2022331) - [Email](mailto:u2022479@giki.edu.pk)  
- **Muhammad Adeel** (Reg# 2022479) - [Email](mailto:u2022331@giki.edu.pk)  
- **Hassan Rais** (Reg# 2022212) - [Email](mailto:u2022212@giki.edu.pk)  
- **Hamza Mehmood Zaidi** (Reg# 2022379) - [Email](mailto:u2022379@giki.edu.pk)  

## Contact  
For any queries, feel free to connect with me on **[LinkedIn](https://www.linkedin.com/in/muhammadadeel21/)**.  

---
ðŸ“Œ *This project was developed as part of our Data Science coursework at GIK Institute, Pakistan.*  
